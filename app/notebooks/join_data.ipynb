{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data_dir: str, ds_set: str, lines: bool):\n",
    "\n",
    "  ds_path = os.path.join(data_dir, ds_set + '.json')\n",
    "  assert os.path.exists(ds_path), f\"{ds_path} doesn't exsits\"\n",
    "\n",
    "  ds = pd.read_json(ds_path, lines=lines)\n",
    "  ds['instructions'] = ds['content']\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " DATA_DIR = \"/mnt/hackney/human_data\"\n",
    " train = load_data(DATA_DIR, 'train', lines=True)\n",
    " valid = load_data(DATA_DIR, 'dev', lines=True)\n",
    " test = load_data(DATA_DIR, 'test', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6738fa6b38cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mnew_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'landmarks_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mhuman_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir_geo = \"/home/onlp_gcp_biu/cabby/app\"\n",
    "\n",
    "human_dataset = train\n",
    "human_dataset['landmarks'] = ''\n",
    "\n",
    "\n",
    "for human_dataset, data_type in zip([train, test, valid], ['train', 'dev', 'test']):\n",
    "  geo_datasets_paths = list(set(human_dataset.rvs_path.tolist()))\n",
    "\n",
    "  for geo_dataset_path in geo_datasets_paths:\n",
    "\n",
    "    path_geo = geo_dataset_path.replace(\"/app_instructor\", data_dir_geo)\n",
    "    entities = util.load_entities(path_geo)\n",
    "\n",
    "    keys = human_dataset[human_dataset['rvs_path']==geo_dataset_path].rvs_sample_number.tolist()\n",
    "    entity_by_keys = np.array(entities)[keys]\n",
    "    entity_landmarks_list = []\n",
    "    for entity, key in zip(entity_by_keys, keys):\n",
    "      landmarks = entity.geo_landmarks\n",
    "      landmark_points = [f\"{t}: {util.list_yx_from_point(v.geometry)}\" for t, v in landmarks.items() if v.geometry]\n",
    "      landmark_str = ';'.join(landmark_points)\n",
    "      human_dataset.loc[human_dataset.rvs_sample_number== key, 'landmarks'] = landmark_str\n",
    "\n",
    "\n",
    "\n",
    "  new_dir = os.path.join(DATA_DIR, 'landmarks_data')\n",
    "  if not os.path.exists(new_dir):\n",
    "    os.mkdir(new_dir)\n",
    "  new_path = os.path.join(new_dir, data_type + '.json')\n",
    "\n",
    "  human_dataset.to_json(new_path, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4f7931bd1c7b434da22e6e2a403714cff5dfb0632842e0d408cece9249e76e8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('cabby')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
