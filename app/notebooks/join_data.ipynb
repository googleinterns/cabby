{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data_dir: str, ds_set: str, lines: bool):\n",
    "\n",
    "  ds_path = os.path.join(data_dir, ds_set + '.json')\n",
    "  assert os.path.exists(ds_path), f\"{ds_path} doesn't exsits\"\n",
    "\n",
    "  ds = pd.read_json(ds_path, lines=lines)\n",
    "  ds['instructions'] = ds['content']\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " DATA_DIR = \"/mnt/hackney/human_data\"\n",
    " train = load_data(DATA_DIR, 'train', lines=True)\n",
    " valid = load_data(DATA_DIR, 'dev', lines=True)\n",
    " test = load_data(DATA_DIR, 'test', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train to: /mnt/hackney/human_data/landmarks_data/train2.json\n",
      "Saved dev to: /mnt/hackney/human_data/landmarks_data/dev2.json\n",
      "Saved test to: /mnt/hackney/human_data/landmarks_data/test2.json\n"
     ]
    }
   ],
   "source": [
    "data_dir_geo = \"/home/onlp_gcp_biu/cabby/app\"\n",
    "\n",
    "human_dataset = train\n",
    "human_dataset['landmarks'] = ''\n",
    "\n",
    "\n",
    "for human_dataset, data_type in zip([train, test, valid], ['train', 'dev', 'test']):\n",
    "  geo_datasets_paths = list(set(human_dataset.rvs_path.tolist()))\n",
    "\n",
    "  for geo_dataset_path in geo_datasets_paths:\n",
    "\n",
    "    path_geo = geo_dataset_path.replace(\"/app_instructor\", data_dir_geo)\n",
    "    entities = util.load_entities(path_geo)\n",
    "\n",
    "    keys = human_dataset[human_dataset['rvs_path']==geo_dataset_path].rvs_sample_number.tolist()\n",
    "    entity_by_keys = np.array(entities)[keys]\n",
    "    entity_landmarks_list = []\n",
    "    for entity, key in zip(entity_by_keys, keys):\n",
    "      landmarks = entity.geo_landmarks\n",
    "      landmark_points = [f\"{t}: {util.list_yx_from_point(v.geometry)}\" for t, v in landmarks.items() if v.geometry]\n",
    "      landmark_str = ';'.join(landmark_points)\n",
    "      human_dataset.loc[human_dataset.rvs_sample_number== key, 'landmarks'] = landmark_str\n",
    "      human_dataset.loc[human_dataset.rvs_sample_number== key, 'route'] = str(entity.route)\n",
    "\n",
    "\n",
    "\n",
    "  new_dir = os.path.join(DATA_DIR, 'landmarks_data')\n",
    "  if not os.path.exists(new_dir):\n",
    "    os.mkdir(new_dir)\n",
    "  new_path = os.path.join(new_dir, data_type + '.json')\n",
    "\n",
    "  print (f'Saved {data_type} to: {new_path}')\n",
    "\n",
    "  human_dataset.to_json(new_path, lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4f7931bd1c7b434da22e6e2a403714cff5dfb0632842e0d408cece9249e76e8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('cabby')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
